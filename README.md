# üåê Hybrid Telugu Morphological Tokenizer

## 1. Introduction

This project provides a **hybrid approach** to tokenizing **Telugu text** ‚Äî a morphologically rich and agglutinative language.  
Standard tokenizers often fail to handle **Sandhi** (euphonic assimilation), where multiple morphemes merge into a single word.

For example:

> ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã‡∞®‡∞ø (‚Äúin the states‚Äù) = ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡±Å (‚Äústates‚Äù) + ‡∞≤‡±ã‡∞®‡∞ø (‚Äúin‚Äù)

To solve this, our tokenizer combines:

- A **rule-based Sandhi splitting engine**, and
- A **data-driven BPE (Byte-Pair Encoding)** model.

This hybrid method enables **grammatically aware tokenization**, improving performance in downstream **NLP tasks** like translation, summarization, and sentiment analysis.

---

## 2. The Hybrid Approach

Our methodology uses **three phases** to combine linguistic knowledge with machine learning.

### üß† Phase 1: Rule-Based Sandhi Engine

The script `sandhi_splitter.py` acts as a _grammar brain_.  
It identifies all possible morphological splits for a given word based on extensive **Telugu and Sanskrit Sandhi rules**.

### üìä Phase 2: Frequency-Based Pre-tokenization

The script `frequency_based_tokenizer.py`:

1. Retrieves all potential splits from the Sandhi engine.
2. Validates them using a **frequency-based vocabulary** (`te_full.txt`).
3. Scores and selects the **most probable grammatical split**.
4. Outputs a processed text file: `corpus.pretokenized.txt`.

### üß© Phase 3: BPE Model Training

The pre-tokenized corpus is then used to train a **SentencePiece (BPE)** model.  
This learns morphemes as base tokens, resulting in a **robust and linguistically aware tokenizer**.

---

## 3. Data Requirements

Two external data files are required (not included due to size):

### üìò Vocabulary File ‚Äî `te_full.txt`

- Contains Telugu words with frequencies.
- Format:
  ```
  ‡∞™‡∞¶‡∞Ç 1234
  ‡∞Æ‡∞∞‡±ã‡∞™‡∞¶‡∞Ç 987
  ```
- Place in `data/` directory.

### üìó Corpus File ‚Äî `telugu_corpus.txt`

- Raw Telugu sentences, one per line.
- Example sources: **IndicCorp**, **Telugu Wikipedia Dumps**.
- Place in `data/` directory.

Expected directory structure:

```
data/
‚îú‚îÄ‚îÄ te_full.txt
‚îî‚îÄ‚îÄ telugu_corpus.txt
```

---

## 4. Usage Workflow

### üîπ Step 1: Pre-tokenize the Corpus

```bash
python src/frequency_based_tokenizer.py
```

Generates `data/corpus.pretokenized.txt` with linguistically-aware splits.  
_(Processing time depends on corpus size.)_

### üîπ Step 2: Train the BPE Model

```bash
spm_train     --input=data/corpus.pretokenized.txt     --model_prefix=models/telugu_bpe     --vocab_size=16000     --model_type=bpe     --character_coverage=1.0
```

> Adjust `vocab_size` (e.g., 32000) based on dataset scale.

### üîπ Step 3: Use the Trained Tokenizer

```python
import sentencepiece as spm

# Load trained model
tokenizer = spm.SentencePieceProcessor()
tokenizer.load('models/telugu_bpe.model')

# Tokenize new text
sentence = "‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã‡∞®‡∞ø ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø."
pieces = tokenizer.encode_as_pieces(sentence)

print("Original:", sentence)
print("Tokenized:", pieces)
```

**Expected Output:**

```
Original: ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã‡∞®‡∞ø ‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø.
Tokenized: ['‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡±Å', '‡∞≤‡±ã‡∞®‡∞ø', '‡∞∏‡∞Æ‡∞∏‡±ç‡∞Ø‡∞≤‡±Å', '‡∞®‡±Å', '‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç', '‡∞™‡∞∞‡∞ø‡∞∑‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø', '.']
```
