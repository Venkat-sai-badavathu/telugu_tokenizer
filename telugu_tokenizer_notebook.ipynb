{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# Hybrid Telugu Tokenizer Project\n",
        "\n",
        "This notebook will guide you through the complete process of creating a sophisticated Telugu tokenizer. We will use a hybrid approach that combines:\n",
        "\n",
        "1.  A Rule-Based Sandhi Engine: To intelligently split words based on grammatical rules.\n",
        "2.  A Frequency-Based Vocabulary: To validate the splits and ensure they result in real, meaningful words.\n",
        "3.  A BPE (Byte-Pair Encoding) Model:To learn tokenization patterns from our pre-processed corpus, resulting in a fast and robust final model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# @title ## Phase 1: Setup and Environment\n",
        "\n",
        "import os\n",
        "# --- 1.1: Create Project Directories ---\n",
        "print(\"Creating project folder structure...\")\n",
        "project_root = \"telugu_tokenizer\"\n",
        "data_folder = os.path.join(project_root, \"data\")\n",
        "src_folder = os.path.join(project_root, \"src\")\n",
        "model_folder = os.path.join(project_root, \"model\")\n",
        "\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "os.makedirs(src_folder, exist_ok=True)\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "print(f\"Folders created: {project_root}, {data_folder}, {src_folder}, {model_folder}\")\n",
        "\n",
        "# --- 1.2: Install Dependencies ---\n",
        "print(\"\\nInstalling the 'sentencepiece' library...\")\n",
        "!pip install sentencepiece -q\n",
        "print(\"Installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN2UPcCvy7Tr",
        "outputId": "8e87dad3-8136-4a36-c2d7-2c1ed18431df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating project folder structure...\n",
            "Folders created: telugu_tokenizer, telugu_tokenizer/data, telugu_tokenizer/src, telugu_tokenizer/model\n",
            "\n",
            "Installing the 'sentencepiece' library...\n",
            "Installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title ## Phase 2: Upload Your Data\n",
        "\n",
        "vocab_path = os.path.join(data_folder, \"te_full.txt\")\n",
        "corpus_path = os.path.join(data_folder, \"telugu_corpus.txt\")\n",
        "\n",
        "if os.path.exists(vocab_path) and os.path.exists(corpus_path):\n",
        "    print(\"✅ Success! Both te_full.txt and telugu_corpus.txt are in the correct location.\")\n",
        "else:\n",
        "    print(\"❌ Files not found. Please make sure you have uploaded both files to the 'telugu_tokenizer/data/' directory.\")"
      ],
      "metadata": {
        "id": "EZcbGTsszBLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04dfba7-cbd4-43c6-c8b4-d5b87809c623"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Files not found. Please make sure you have uploaded both files to the 'telugu_tokenizer/data/' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Phase 3.1: Create the Sandhi Splitter (Rule Engine)\n",
        "\n",
        "%%writefile telugu_tokenizer/src/sandhi_splitter.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# తెలుగు అక్షరాల నిర్వచనం\n",
        "telugu_consonants = \"కఖగఘఙచఛజఝఞటఠడఢణతథదధనపఫబభమయరలవశషసహళక్షఱ\"\n",
        "virama = '్'\n",
        "vowels = \"అఆఇఈఉఊఋౠఎఏఐఒఓఔ\"\n",
        "matra_map = {\n",
        "    'ా': 'ఆ', 'ి': 'ఇ', 'ీ': 'ఈ', 'ు': 'ఉ', 'ూ': 'ఊ', 'ృ': 'ఋ', 'ౄ': 'ౠ',\n",
        "    'ె': 'ఎ', 'ే': 'ఏ', 'ై': 'ఐ', 'ొ': 'ఒ', 'ో': 'ఓ', 'ౌ': 'ఔ'\n",
        "}\n",
        "vowel_to_matra = {v: k for k, v in matra_map.items()}\n",
        "\n",
        "def find_consonant_cluster_start(word, end_index):\n",
        "    \"\"\"Helper to find the start of a consonant cluster before a given index.\"\"\"\n",
        "    start_index = -1\n",
        "    j = end_index - 1\n",
        "    while j >= 0:\n",
        "        current_char = word[j]\n",
        "        if current_char in telugu_consonants:\n",
        "            if j == 0 or word[j-1] != virama:\n",
        "                start_index = j\n",
        "                break\n",
        "        elif current_char != virama:\n",
        "            break\n",
        "        j -= 1\n",
        "    return start_index\n",
        "\n",
        "# --- తెలుగు సంధులు ---\n",
        "\n",
        "def split_savarnadeergha(word):\n",
        "    \"\"\"సవర్ణదీర్ఘ సంధి నియమాల ప్రకారం పదాన్ని విడదీస్తుంది.\"\"\"\n",
        "    sandhi_map = {'ా': ('అ', ''), 'ీ': ('ఇ', 'ి'), 'ూ': ('ఉ', 'ు'), 'ౄ': ('ఋ', 'ృ')}\n",
        "    splits = []\n",
        "    for i, char in enumerate(word):\n",
        "        if char in sandhi_map:\n",
        "            start_of_cluster = find_consonant_cluster_start(word, i)\n",
        "            if start_of_cluster != -1:\n",
        "                short_vowel_char, short_matra = sandhi_map[char]\n",
        "                prefix = word[:start_of_cluster]\n",
        "                consonant = word[start_of_cluster:i]\n",
        "                word1 = prefix + consonant + short_matra if short_matra else prefix + consonant\n",
        "                word2 = short_vowel_char + word[i+1:]\n",
        "                splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_utva_ikara_sandhi(word):\n",
        "    \"\"\"ఉకార/ఇకార (లోప) సంధి ప్రకారం విడదీస్తుంది. (ఉదా: రాముడితడు)\"\"\"\n",
        "    splits = []\n",
        "    possible_elided_matras = {'ు': 'ఉ', 'ి': 'ఇ'}\n",
        "    for i in range(1, len(word)):\n",
        "        current_char = word[i]\n",
        "        if current_char in matra_map:\n",
        "            start_of_cluster = find_consonant_cluster_start(word, i)\n",
        "            if start_of_cluster != -1:\n",
        "                prefix = word[:start_of_cluster]\n",
        "                consonant = word[start_of_cluster:i]\n",
        "                word2_start_vowel = matra_map[current_char]\n",
        "                word2 = word2_start_vowel + word[i+1:]\n",
        "                for matra, vowel in possible_elided_matras.items():\n",
        "                    word1 = prefix + consonant + matra\n",
        "                    splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_yadagama_sandhi(word):\n",
        "    \"\"\"యడాగమ సంధి ప్రకారం విడదీస్తుంది. (ఉదా: మణియంత)\"\"\"\n",
        "    splits = []\n",
        "    for i in range(1, len(word) - 1):\n",
        "        if word[i] == 'య' and i > 1 and word[i-1] == 'ి' and word[i-2] in telugu_consonants:\n",
        "            if (i + 1 < len(word)) and (word[i+1] not in matra_map and word[i+1] in telugu_consonants):\n",
        "                 word1 = word[:i]\n",
        "                 word2 = \"అ\" + word[i+1:]\n",
        "                 splits.append(f\"'{word1}' + '{word2}'\")\n",
        "            elif (i + 1 < len(word)) and word[i+1] in matra_map:\n",
        "                 word1 = word[:i]\n",
        "                 word2 = matra_map[word[i+1]] + word[i+2:]\n",
        "                 splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_amredita_sandhi(word):\n",
        "    \"\"\"ఆమ్రేడిత సంధి ప్రకారం విడదీస్తుంది. (ఉదా: ఔరౌర)\"\"\"\n",
        "    splits = []\n",
        "    mid = len(word) // 2\n",
        "    if len(word) > 1 and len(word) % 2 == 0 and word[:mid] == word[mid:]:\n",
        "        splits.append(f\"'{word[:mid]}' + '{word[mid:]}'\")\n",
        "    return splits\n",
        "\n",
        "def split_gasada_dava_adesa(word):\n",
        "    \"\"\"గసడదవాదేశ సంధి ప్రకారం విడదీస్తుంది. (ఉదా: వాడుగొట్టె)\"\"\"\n",
        "    splits = []\n",
        "    reverse_map = {'గ': 'క', 'స': 'చ', 'డ': 'ట', 'ద': 'త', 'వ': 'ప'}\n",
        "    for i, char in enumerate(word):\n",
        "        if char in reverse_map:\n",
        "            start_of_cluster = find_consonant_cluster_start(word, i)\n",
        "            if start_of_cluster != -1:\n",
        "                word1_candidate = word[:start_of_cluster]\n",
        "                if len(word1_candidate) > 1:\n",
        "                    word1 = word1_candidate\n",
        "                    original_consonant = reverse_map[char]\n",
        "                    suffix = word[start_of_cluster:]\n",
        "                    word2 = original_consonant + suffix[1:]\n",
        "                    splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_trika(word):\n",
        "    \"\"\"త్రికసంధి ప్రకారం విడదీస్తుంది. (ఉదా: అక్కన్య)\"\"\"\n",
        "    splits = []\n",
        "    trika_vowels = {'అ': 'ఆ', 'ఇ': 'ఈ', 'ఎ': 'ఏ'}\n",
        "    if len(word) > 3 and word[0] in trika_vowels:\n",
        "        if word[2] == '్' and word[1] == word[3]:\n",
        "            word1 = trika_vowels[word[0]]\n",
        "            word2 = word[3:]\n",
        "            splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "# --- సంస్కృత సంధులు ---\n",
        "\n",
        "def split_guna(word):\n",
        "    \"\"\"గుణ సంధి ప్రకారం విడదీస్తుంది.\"\"\"\n",
        "    sandhi_map = {'ే': ('ఇ', 'ఈ'), 'ో': ('ఉ', 'ఊ')}\n",
        "    splits = []\n",
        "    for i, char in enumerate(word):\n",
        "        if char in sandhi_map:\n",
        "            start_of_cluster = find_consonant_cluster_start(word, i)\n",
        "            if start_of_cluster != -1:\n",
        "                prefix = word[:start_of_cluster]\n",
        "                consonant = word[start_of_cluster:i]\n",
        "                word1_a = prefix + consonant\n",
        "                vowel1, vowel2 = sandhi_map[char]\n",
        "                suffix = word[i+1:]\n",
        "                splits.append(f\"'{word1_a}' + '{vowel1 + suffix}'\")\n",
        "                splits.append(f\"'{word1_a}' + '{vowel2 + suffix}'\")\n",
        "    return splits\n",
        "\n",
        "def split_vriddhi(word):\n",
        "    \"\"\"వృద్ధి సంధి ప్రకారం విడదీస్తుంది.\"\"\"\n",
        "    sandhi_map = {'ై': ('ఏ', 'ఐ'), 'ౌ': ('ఓ', 'ఔ')}\n",
        "    splits = []\n",
        "    for i, char in enumerate(word):\n",
        "        if char in sandhi_map:\n",
        "            start_of_cluster = find_consonant_cluster_start(word, i)\n",
        "            if start_of_cluster != -1:\n",
        "                prefix = word[:start_of_cluster]\n",
        "                consonant = word[start_of_cluster:i]\n",
        "                word1_a = prefix + consonant\n",
        "                vowel1, vowel2 = sandhi_map[char]\n",
        "                suffix = word[i+1:]\n",
        "                splits.append(f\"'{word1_a}' + '{vowel1 + suffix}'\")\n",
        "                splits.append(f\"'{word1_a}' + '{vowel2 + suffix}'\")\n",
        "    return splits\n",
        "\n",
        "def split_yanadesa(word):\n",
        "    \"\"\"యణాదేశ సంధి ప్రకారం విడదీస్తుంది.\"\"\"\n",
        "    splits = []\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] == '్' and i + 1 < len(word):\n",
        "            base_consonant_char = word[i-1]\n",
        "            adesa_char = word[i+1]\n",
        "\n",
        "            original_matra = None\n",
        "            if adesa_char == 'య': original_matra = 'ి'\n",
        "            elif adesa_char == 'వ': original_matra = 'ు'\n",
        "            elif adesa_char == 'ర': original_matra = 'ృ'\n",
        "\n",
        "            if original_matra:\n",
        "                cluster_and_matra = word[i+2:]\n",
        "                next_char = cluster_and_matra[0] if cluster_and_matra else ''\n",
        "                vowel_sound = matra_map.get(next_char, 'అ')\n",
        "\n",
        "                word1 = word[:i-1] + base_consonant_char + original_matra\n",
        "                word2 = vowel_sound + (cluster_and_matra[1:] if next_char in matra_map else cluster_and_matra)\n",
        "                splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_jashtva(word):\n",
        "    \"\"\"జశ్త్వ సంధి ప్రకారం విడదీస్తుంది. (ఉదా: వాగీశుడు)\"\"\"\n",
        "    splits = []\n",
        "    reverse_map = {'గ': 'క', 'జ': 'చ', 'డ': 'ట', 'ద': 'త', 'బ': 'ప'}\n",
        "    for i, char in enumerate(word):\n",
        "        if char in reverse_map:\n",
        "            vowel_of_char = 'అ'\n",
        "            suffix_start_index = i + 1\n",
        "            if i + 1 < len(word) and word[i+1] in matra_map:\n",
        "                vowel_of_char = matra_map[word[i+1]]\n",
        "                suffix_start_index = i + 2\n",
        "\n",
        "            word1 = word[:i] + reverse_map[char] + virama\n",
        "            word2 = vowel_of_char + word[suffix_start_index:]\n",
        "            splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_schutva(word):\n",
        "    \"\"\"శ్చుత్వ సంధి ప్రకారం విడదీస్తుంది. (ఉదా: సచ్చరిత్ర)\"\"\"\n",
        "    splits = []\n",
        "    if 'చ్చ' in word:\n",
        "        index = word.find('చ్చ')\n",
        "        word1 = word[:index] + 'త్'\n",
        "        word2 = 'చ' + word[index+2:]\n",
        "        splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    if 'శ్శ' in word:\n",
        "        index = word.find('శ్శ')\n",
        "        word1 = word[:index] + 'స్'\n",
        "        word2 = 'శ' + word[index+2:]\n",
        "        splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "def split_anunasika(word):\n",
        "    \"\"\"అనునాసిక సంధి ప్రకారం విడదీస్తుంది. (ఉదా: జగన్నాథుడు)\"\"\"\n",
        "    splits = []\n",
        "    reverse_map = {'ఙ': 'క', 'ఞ': 'చ', 'ణ': 'ట', 'న': 'త', 'మ': 'ప'}\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] == '్' and word[i-1] in reverse_map:\n",
        "            nasal = word[i-1]\n",
        "            word1 = word[:i-1] + reverse_map[nasal] + virama\n",
        "            word2 = word[i+1:]\n",
        "            splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    if 'న్న' in word:\n",
        "        index = word.find('న్న')\n",
        "        word1 = word[:index] + 'త్'\n",
        "        word2 = 'న' + word[index+2:]\n",
        "        splits.append(f\"'{word1}' + '{word2}'\")\n",
        "    return splits\n",
        "\n",
        "\n",
        "def find_all_splits(word):\n",
        "    \"\"\"అన్ని సంధి ఫంక్షన్‌లను అమలు చేస్తుంది.\"\"\"\n",
        "    all_splits = {\n",
        "        \"సవర్ణదీర్ఘ సంధి\": split_savarnadeergha(word),\n",
        "        \"గుణ సంధి\": split_guna(word),\n",
        "        \"వృద్ధి సంధి\": split_vriddhi(word),\n",
        "        \"యణాదేశ సంధి\": split_yanadesa(word),\n",
        "        \"ఉత్వ/ఇత్వ సంధి (లోప)\": split_utva_ikara_sandhi(word),\n",
        "        \"యడాగమ సంధి\": split_yadagama_sandhi(word),\n",
        "        \"గసడదవాదేశ సంధి\": split_gasada_dava_adesa(word),\n",
        "        \"ఆమ్రేడిత సంధి\": split_amredita_sandhi(word),\n",
        "        \"త్రిక సంధి\": split_trika(word),\n",
        "        \"జశ్త్వ సంధి\": split_jashtva(word),\n",
        "        \"శ్చుత్వ సంధి\": split_schutva(word),\n",
        "        \"అనునాసిక సంధి\": split_anunasika(word),\n",
        "    }\n",
        "    return {sandhi: list(set(splits)) for sandhi, splits in all_splits.items() if splits}\n",
        "\n"
      ],
      "metadata": {
        "id": "R1vKVbyQzMK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47df1ef-13ec-4f6a-9b25-89ee1908b84c",
        "cellView": "form"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting telugu_tokenizer/src/sandhi_splitter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import sys\n",
        "\n",
        "project_root = \"/content/telugu_tokenizer\"\n",
        "src_path = os.path.join(project_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "from sandhi_splitter import find_all_splits\n",
        "\n",
        "def load_vocabulary_with_frequency(filepath):\n",
        "    \"\"\"\n",
        "    Loads the vocabulary from the user-provided file (e.g., 'te_full.txt').\n",
        "    It parses each line to get the word and its frequency, storing them\n",
        "    in a dictionary for fast lookups.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Vocabulary file not found at: {filepath}\")\n",
        "\n",
        "    vocab = {}\n",
        "    print(f\"Loading vocabulary from '{filepath}'...\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                word = ' '.join(parts[:-1])\n",
        "                freq_str = parts[-1]\n",
        "\n",
        "                if freq_str.isdigit():\n",
        "                    vocab[word] = int(freq_str)\n",
        "                else:\n",
        "\n",
        "                    vocab[line] = 1\n",
        "            else:\n",
        "\n",
        "                vocab[line] = 1\n",
        "\n",
        "    print(f\"Loaded {len(vocab)} words into the vocabulary.\")\n",
        "    return vocab\n",
        "\n",
        "def get_best_frequency_split(word, vocab):\n",
        "    \"\"\"\n",
        "    This is the core intelligent function. It gets all possible rule-based splits\n",
        "    and then validates them against the loaded vocabulary. It chooses the best split\n",
        "    based on the frequency of the resulting morphemes.\n",
        "    \"\"\"\n",
        "    possible_splits = find_all_splits(word)\n",
        "    if not possible_splits:\n",
        "        return [word]\n",
        "\n",
        "    valid_splits = []\n",
        "    for sandhi_type, splits in possible_splits.items():\n",
        "        for split_str in splits:\n",
        "            parts = [p.strip().strip(\"'\") for p in split_str.split('+')]\n",
        "\n",
        "            if len(parts) == 2:\n",
        "                part1, part2 = parts\n",
        "\n",
        "                p1_in_vocab = part1 in vocab\n",
        "                p2_in_vocab = part2 in vocab\n",
        "\n",
        "                score = 0\n",
        "                if p1_in_vocab and p2_in_vocab:\n",
        "                    score = 1000 + min(vocab.get(part1, 1), vocab.get(part2, 1))\n",
        "                elif p1_in_vocab or p2_in_vocab:\n",
        "\n",
        "                    score = 100 + (vocab.get(part1, 1) if p1_in_vocab else vocab.get(part2, 1))\n",
        "\n",
        "                if score > 0:\n",
        "                    valid_splits.append({'parts': [part1, part2], 'score': score, 'type': sandhi_type})\n",
        "\n",
        "    if not valid_splits:\n",
        "        return [word]\n",
        "\n",
        "    # Choose the split with the highest score\n",
        "    best_split = max(valid_splits, key=lambda x: x['score'])\n",
        "    return best_split['parts']\n",
        "\n",
        "def pre_tokenize_corpus(input_filepath, output_filepath, vocab):\n",
        "    \"\"\"\n",
        "    Reads a corpus file line by line, applies the smart splitting to each word,\n",
        "    and writes the pre-tokenized sentences to an output file.\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting pre-tokenization of '{input_filepath}'...\")\n",
        "    with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "        total_lines = 0\n",
        "        try:\n",
        "            with open(input_filepath, 'r', encoding='utf-8') as f_count:\n",
        "                total_lines = sum(1 for line in f_count)\n",
        "        except Exception:\n",
        "            total_lines = -1\n",
        "\n",
        "        infile.seek(0)\n",
        "\n",
        "        for i, line in enumerate(infile):\n",
        "            words = line.strip().split()\n",
        "            pre_tokenized_words = []\n",
        "            for word in words:\n",
        "                # Also split words connected by non-breaking space or other common joiners\n",
        "                sub_words = word.replace('\\u200c', ' ').split()\n",
        "                for sub_word in sub_words:\n",
        "                    split_parts = get_best_frequency_split(sub_word, vocab)\n",
        "                    pre_tokenized_words.extend(split_parts)\n",
        "\n",
        "            outfile.write(' '.join(pre_tokenized_words) + '\\n')\n",
        "\n",
        "            progress_msg = f\"  ...processed {i+1} lines\"\n",
        "            if total_lines > 0:\n",
        "                progress_msg = f\"  ...processed {i+1} of {total_lines} lines\"\n",
        "\n",
        "            if (i + 1) % 500 == 0 or (total_lines > 0 and (i + 1) == total_lines):\n",
        "                print(progress_msg)\n",
        "\n",
        "    print(f\"Pre-tokenization complete. Output saved to '{output_filepath}'.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
        "    project_root_main = \"telugu_tokenizer\"\n",
        "    vocab_file = os.path.join(project_root_main, \"data\", \"te_full.txt\")\n",
        "    input_corpus = os.path.join(project_root_main, \"data\", \"telugu_corpus.txt\")\n",
        "    output_corpus = os.path.join(project_root_main, \"data\", \"corpus.pretokenized.txt\")\n",
        "\n",
        "    try:\n",
        "        vocabulary = load_vocabulary_with_frequency(vocab_file)\n",
        "        pre_tokenize_corpus(input_corpus, output_corpus, vocabulary)\n",
        "\n",
        "        print(\"\\n--- Pipeline Finished ---\")\n",
        "        print(f\"The pre-tokenized corpus is ready at: '{output_corpus}'\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please make sure your data files are uploaded to the correct directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HLeOapPGq8T",
        "outputId": "ce5a9d2e-b42d-48db-afd9-df2b2bdc0571",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Vocabulary file not found at: telugu_tokenizer/data/te_full.txt\n",
            "Please make sure your data files are uploaded to the correct directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Phase 3.3: Confirm File Creation\n",
        "#\n",
        "# Let's quickly verify that both Python modules were created successfully in the `src` directory.\n",
        "\n",
        "import os\n",
        "src_folder = \"telugu_tokenizer/src\"\n",
        "sandhi_file = os.path.join(src_folder, \"sandhi_splitter.py\")\n",
        "tokenizer_file = os.path.join(src_folder, \"frequency_based_tokenizer.py\")\n",
        "\n",
        "if os.path.exists(sandhi_file):\n",
        "  print(f\" Success! sandhi_splitter.py has been created.\")\n",
        "else:\n",
        "  print(f\" Error! sandhi_splitter.py was not created.\")\n",
        "\n",
        "if os.path.exists(tokenizer_file):\n",
        "  print(f\" Success! frequency_based_tokenizer.py has been created.\")\n",
        "else:\n",
        "  print(f\"Error! frequency_based_tokenizer.py was not created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPmDbo3VG0Ur",
        "outputId": "b9f41305-9d3b-4fc8-9242-47f8b0cb945e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Success! sandhi_splitter.py has been created.\n",
            "Error! frequency_based_tokenizer.py was not created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Phase 4: Run the Pre-tokenization Pipeline\n",
        "#\n",
        "# Now we execute the script we just created.\n",
        "#\n",
        "# It will:\n",
        "# 1. Load the vocabulary from `te_full.txt`.\n",
        "# 2. Read your `telugu_corpus.txt`.\n",
        "# 3. Apply the Sandhi rules and validate the splits for every word.\n",
        "# 4. Write the results to a new file: `corpus.pretokenized.txt`.\n",
        "#\n",
        "# This step might take some time depending on the size of your corpus.\n",
        "\n",
        "!python telugu_tokenizer/src/frequency_based_tokenizer.py\n"
      ],
      "metadata": {
        "id": "0I3PVAtZzSzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16938a3b-5e56-4e35-ee84-05adc125ce53",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/telugu_tokenizer/src/frequency_based_tokenizer.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title ## Phase 5: Train the BPE Tokenizer Model\n",
        "#\n",
        "# With our grammatically pre-processed corpus ready, we can now train the final SentencePiece model.\n",
        "# This model will learn the morphemes we have identified.\n",
        "#\n",
        "# The output will be two files saved in the `model` directory:\n",
        "# * `telugu_bpe.model`: The trained tokenizer model.\n",
        "# * `telugu_bpe.vocab`: A human-readable vocabulary file.\n",
        "\n",
        "print(\"Starting SentencePiece model training...\")\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "input_file = os.path.join(data_folder, \"corpus.pretokenized.txt\")\n",
        "model_prefix = os.path.join(model_folder, \"telugu_bpe\")\n",
        "vocab_size = 16000 # This is a good starting point, can be tuned.\n",
        "model_type = \"bpe\"\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f'--input={input_file} '\n",
        "    f'--model_prefix={model_prefix} '\n",
        "    f'--vocab_size={vocab_size} '\n",
        "    f'--model_type={model_type} '\n",
        "    f'--character_coverage=1.0'\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Training complete! Your tokenizer model is saved at '{model_prefix}.model'\")"
      ],
      "metadata": {
        "id": "6WUHxLNizaoB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "6f9b3eef-c198-4c7a-80c8-065355e5e846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SentencePiece model training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Not found: \"telugu_tokenizer/data/corpus.pretokenized.txt\": No such file or directory Error #2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3993441324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bpe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m spm.SentencePieceTrainer.train(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34mf'--input={input_file} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34mf'--model_prefix={model_prefix} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_LogStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mostream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;34m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromString\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"telugu_tokenizer/data/corpus.pretokenized.txt\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title ## Phase 6: Test Your New Tokenizer\n",
        "#\n",
        "# Let's see our new tokenizer in action! We will load the model we just trained\n",
        "# and use it to tokenize some sample sentences.\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Load the trained model\n",
        "tokenizer = spm.SentencePieceProcessor()\n",
        "tokenizer.load(f'{model_prefix}.model')\n",
        "\n",
        "# --- Test Sentences ---\n",
        "test_sentences = [\n",
        "    \"రామాలయం చాలా అందంగా ఉంది.\",\n",
        "    \"ప్రభుత్వం రాష్ట్రాలలోని సమస్యలను పరిష్కరించింది.\",\n",
        "    \"అతను అత్యంత ముఖ్యమైన వ్యక్తి.\",\n",
        "    \"విద్యార్థి పాఠశాలకు వెళ్ళాడు.\",\n",
        "    \"ఔరౌర, ఎంత గొప్ప ప్రదర్శన!\",\n",
        "    \"సీతా దేవి ఉదయాన్నే తోటలో నడిచింది.\",\n",
        "    \"ఈ రోజు వాతావరణం చల్లగా ఉంది.\",\n",
        "    \"మన సంస్కృతి ప్రపంచంలో ప్రత్యేక స్థానం పొందింది.\",\n",
        "    \"రాత్రి ఆకాశంలో నక్షత్రాలు మెరుస్తున్నాయి.\",\n",
        "    \"పిల్లలు ఆనందంగా ఆటలు ఆడుతున్నారు.\",\n",
        "    \"పుస్తకం చదివి కొత్త విషయాలు నేర్చుకున్నాను.\",\n",
        "    \"రాముడు మరియు లక్ష్మణుడు అడవికి వెళ్లారు.\",\n",
        "    \"చిన్న పక్షి చెట్టుపైన గూడు కట్టింది.\",\n",
        "    \"మేఘాలు కమ్ముకొని వర్షం కురిసింది.\",\n",
        "    \"ఆమె చిరునవ్వు అందరినీ ఆకట్టుకుంది.\"\n",
        "]\n",
        "\n",
        "\n",
        "print(\"--- Tokenizer Test Results ---\")\n",
        "for sentence in test_sentences:\n",
        "    print(f\"\\nOriginal:  {sentence}\")\n",
        "    tokenized = tokenizer.encode_as_pieces(sentence)\n",
        "    # The ' ' character represents a space.\n",
        "    print(f\"Tokenized: {tokenized}\")"
      ],
      "metadata": {
        "id": "eYz6X87OzeX6",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCDpzCPyKoi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title ## Phase 7: Save Your Work\n",
        "#\n",
        "# The Colab environment is temporary. To save your trained model and the important\n",
        "# data files for future use, run this cell. It will download the files to your local computer.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading the trained tokenizer model...\")\n",
        "files.download(f'{model_prefix}.model')\n",
        "\n",
        "print(\"\\nDownloading the vocabulary file...\")\n",
        "files.download(f'{model_prefix}.vocab')\n",
        "\n",
        "print(\"\\nDownloading the pre-tokenized corpus...\")\n",
        "files.download(os.path.join(data_folder, \"corpus.pretokenized.txt\"))\n",
        "\n",
        "print(\"\\n✅ All essential files have been downloaded.\")\n",
        "\n",
        "# --- End of Notebook ---\n"
      ]
    }
  ]
}